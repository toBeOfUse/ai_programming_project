## Neural Network

By Mitch Jacovetty. Written for Python 3.11 with numpy as its only external dependency.

After installing numpy with `pip install numpy --user`, run with `python neural_network.py configuration.json`. Any data not found in configuration.json will be read from the terminal when the program runs.

By default, the network is trained on a simple rounding function that returns 0 for numbers in the range [0, 0.5] and 1 for numbers in the range (0.5, 1]. This function fits well with the output capabilities of the sigmoid function as a simple test case for the algorithm functioning.

The neural network algorithm works like this: there is an input vector to the network, and an output vector, and in between the neural network transfers weighted combinations of values from neuron to neuron until the output is obtained. The transfer of data between layers of neurons is implemented as matrix multiplication.

If you had an input vector of length 2 and you wanted it to transfer it to a hidden layer of 3 neurons, you would find the input to the hidden layer by multiplying each original input value with its corresponding weight and adding the results together. Simple.

```
input_to_neuron_1 = input[0]*weights_for_1[0] + input[1]*weights_for_1[1]
input_to_neuron_2 = input[0]*weights_for_2[0] + input[1]*weights_for_2[1]
input_to_neuron_3 = input[0]*weights_for_3[0] + input[1]*weights_for_3[1]
```

This process of multiplying corresponding elements and summing the results is called the "dot product" when dealing with vectors. Matrix multiplication is done using dot products to get the values in the result. Specifically, if you take an nxm matrix times an mx1 column vector, you will get an nx1 matrix. The first element in the result is the first row of the matrix times the column vector (using the dot product). The second element is the second row of the matrix times the column vector. The third element is the third row of the matrix times the column vector. And so on.

As you can see, for each of the elements of the output, the result is the dot product of something new (a given matrix row) times the column vector; the column vector stays the same. This parallels how our neural network works; when determining the input to any given neuron, we always need the input vector in our multiplication, but the weights change for each neuron. To fully apply matrix multiplication to neuron weighting, we can store the weights for neurons in the rows of the matrix. The input, expressed as a column vector, will be multiplied (with the dot product) with each of them in turn, thus producing as many outputs as there are rows of the matrix.

So: with matrix multiplication mxn \* nx1, we get an mx1 column vector. We've taken n values and weighted and summed them to turn them into m values. Therefore, this can be used to transfer data from a neural network layer of size n to one of size m.

Using this logic, we can transfer data from the input layer to the hidden layer, then with square matrices (because the number of inputs and outputs are the same within the hidden layer) between the hidden layers, then with an appropriately sized matrix from the last hidden layer to the output layer. (As we have seen, we'll need a matrix with as many columns as there are neurons in the hidden layer and as many rows as there are neurons in the output layer.) In between, we are adding a bias to get the total input, checking if that total input is greater than the node's threshold, and outputting the value `activation(total_input)` from that neuron if so. The activation function in this program is the sigmoid function.

So, that's how you feed inputs through the neural network to get the output. Before you can get useful values from doing that, though, you must train the neural network. This program takes the first 1/5th of the input data as "testing data", which is simply run through the network to compute its average error at that point. The rest of the data is used for training; it is fed through the matrix in the same way, but then the difference between its expected and actual output is computed, and this is used to correct the weights in the matrix through backpropagation of error.

To understand how this works, it's helpful to think of the output of a neural network as one gigantic function. At the first layer, the output of a neuron is as we've described, `activation(weights*input + bias)`; then, that in itself becomes a new input to the next layer: `activation(weights * activation(weights*input + bias) + bias)`. Then, that goes into its place as the input for the next layer. The final output for one of the output neurons is a gigantic nested function. Then, all the outputs go into the error function. It's tempting to think of the important variable in the error function as being the input data for the neural network, but during training, the input is fixed; the input to the function that we can change is the weights. We can use the partial derivative of the error function to determine its slope for any given weight; if the slope is positive, then increasing the weight will make it go up; if the slope is negative, then increasing the weight will make it go down. We want to decrease the error, so we decrease a given weight if the partial derivative of the error function given that weight is positive and increase it if its negative. For any given training example, we only want to change the weights a small amount, since all we know is what will decrease the error for that example and we want to accumulate the changes that all the examples suggest; the amount that we change the weight is determined by the learning rate.

Calculus tells us that the rule for determining the derivative for nested functions is the chain rule, which states: the derivative of f(g(x)) = f'(g(x))g'(x). To determine the derivative of the weights in the last layer, we can write it like: the derivative of `error(activation(input_from_previous_layer))` for each neuron is `error'(activation(input_from_previous_layer)) * activation'(input_from_previous_layer)`. During back propagation, we are using an activation function and an error function with simple derivatives. We can also store each input from each layer when feeding data through the network initially. Therefore, it is easy to determine the derivative of the error function and adjust the weights between the last layers in the correct direction.

Moving backwards through the neural network is also not too hard. (If you instead try to start at the beginning of the network and correct the errors moving forward, you will have a very difficult time; for one thing, you need the error to feed it into the derivative of the cost function at the beginning because of the chain rule.) The cost function is only applied to the outer layer, so we don't use it or its derivative again. Instead, we apply the chain rule to the next "layer" of nested functions.
